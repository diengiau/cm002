---
title: "Test 03 - Regression diagnostics and tidy regression results"
author: "Richard"
date: "2024"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load required packages

```{r}
#| echo: true
#| warning: false
library(tidyverse) 
# if you're using macOS, you can run: library(dplyr)
library(skimr)
library(broom)
library(modelr)
```

## Prepare Data

```{r}
#| echo: true
#| warning: false
Hsb = read_csv("data/raw/hsb.csv")
Hsb = Hsb %>% 
  mutate(
    race = as.factor(race),
    schtyp = as.factor(schtyp),
    prog = as.factor(prog)
  )
```

## A regression

Recall that we run a regerssion between `write` score on `read` score and `female` (equal 1 for female students):

```{r}
#| echo: true
ols_reg_fit = lm(formula = write ~ read + female, data = Hsb)
summary(ols_reg_fit)
```

## Tidy the coefficients

-   The about regressions results are in text format, which is time-consuming to copy to a report
-   How about we transform it into a dataframe to easy to manipulate later: use function `tidy` from `broom` package
-   For example, if we want to get the coefficient of `read`, we can easy `filter` and `select` to get the coefficient, rather than copy-and-paste:

```{r}
#| echo: true
tidy(ols_reg_fit)
```

## Get predictions and residuals

-   Recall that in a regression

$$Y = a + bX + e$$

-   So the prediction is:

$$\hat{Y} = \hat{a} + \hat{b}X$$

-   and residuals:

$$\hat{e} = Y - \hat{Y}$$

-   We have several ways to get the predictions and residuals

## 1st way: manual calculation

The fitted Y is the product of estimated coefficients and the corresponding X.

```{r}
#| echo: true
write_hat = 20.2283684 + 0.5658869*Hsb$read + 5.4868940*Hsb$female
head(write_hat)
```

The residuals is the difference between Y and fitted Y:

```{r}
head(Hsb$write - write_hat)
```

## 2nd way: use `tidy::augment`

This function added several new columns, including the fitted and residuals to the original data. Compare the results to the manual calculation above.

```{r}
#| echo: true
Hsb = augment(ols_reg_fit, Hsb) 
Hsb %>% 
  select(.fitted:.std.resid) %>% 
  head()
```

## Training vs Test sample

-   We often split our data into training vs test sample:

    -   Training sample: to train historical data

    -   Test sample: new data to make prediction

-   e.g., Netflix uses our historical watched movies to recommend our next movies to watch

## Re-train our case

```{r}
# train: use the first 150 obs
ols_reg_fit = lm(formula = write ~ read + female, data = Hsb[1:150,])
# test: use the last 50 obs 
augment(ols_reg_fit, newdata = Hsb[151:200,])
```

## Regression diagnostics

-   The OLS regressions have several important assumptions, which we assume the data must be, to make sure the estimation is correct.

-   Thus, after running regression, we often need to check these assumptions again to make sure

-   This process is called as "regression diagnostics"

-   I borrow a lot from [this slide note from UCLA](https://stats.idre.ucla.edu/wp-content/uploads/2019/02/R_reg_part2.html)

## Assumption 1: Homogeneity of variance (homoscedasticity)

-   It assumes that the variance of residuals is constant

-   If the model is well-fitted, there should be no pattern to the residuals plotted against the fitted values.

-   Let's plot to see:

## Plot of residuals

```{r}
#| echo: true
plot(Hsb$.resid ~ Hsb$.fitted)
abline(h = 0, lty = 2)
```

## Assumption 2: Normality of residuals

-   It assumes that the residuals follow a normal distribution

-   Thus, we need to test normality for the residuals

## Q-Q plot

```{r}
#| echo: true
qqnorm(Hsb$.resid)
qqline(Hsb$.resid)
```

## Normality test for residuals

Do you remember we have a test for normality?

## Assumption 3: Check for multicolinearity

-   The term collinearity implies that two variables are near perfect linear combinations of one another.

-   VIF, variance inflation factor, is used to measure the degree of multicollinearity.

-   Rule-of-thump: VIF \>= 10 means that the variable could be considered as a linear combination of other independent variables.

## Multicolinearity check in R

-   Install `car` package if not yet

```{r}
#| echo: true
# install.packages("car")
car::vif(ols_reg_fit)

```

-   All coefficients have low VIF
    -   Less concern on multicolinearity problem

## Quiz time

Hmm...
